{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cffl/bert-base-styleclassification-subjective-neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(json_file_path, max_length=512):\n",
    "    processed_sentences = []\n",
    "    \n",
    "    # read json file\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        sentences_data = json.load(file)\n",
    "    \n",
    "    # iterate each sentence\n",
    "    for entry in sentences_data:\n",
    "        sentence = entry['sentence']\n",
    "        \n",
    "        # tokenize and truncate\n",
    "        tokens = tokenizer(sentence, truncation=True, max_length=512, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # convert to text\n",
    "        truncated_sentence = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
    "        \n",
    "        # save processed sentence\n",
    "        processed_sentences.append(truncated_sentence)\n",
    "    \n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classify = pipeline(\n",
    "    task=\"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    top_k=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"All around the rim of China the nations of non-Communist Asia are building a new prosperity and developing a new cohesiveness, which together suggest that they should be able to play far more assertive roles in their own defense.\\n The Soviet drive for strategic supremacy — which the Soviets already have very nearly achieved, while the United States has passively watched — is deeply troubling and seriously threatening\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"Since China joined the WTO, Americans have witnessed the closure of more than 50,000 factories and the loss of tens of millions of jobs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': '4 stars', 'score': 0.34373730421066284}, {'label': '5 stars', 'score': 0.2400761842727661}, {'label': '3 stars', 'score': 0.17142046988010406}, {'label': '2 stars', 'score': 0.1672779768705368}, {'label': '1 star', 'score': 0.07748813927173615}]]\n"
     ]
    }
   ],
   "source": [
    "scores = classify(text)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 star\n"
     ]
    }
   ],
   "source": [
    "scores2 = classify(text2)\n",
    "print(scores2[0][0]['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(scores[0][0]['label'].split(' ')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./speech_json/37_nixon_speech.json', 'r') as file:\n",
    "    nixon_speech = json.load(file)\n",
    "\n",
    "with open('./speech_json/38_ford_speech.json', 'r') as file:\n",
    "    ford_speech = json.load(file)\n",
    "\n",
    "with open('./speech_json/39_carter_speech.json', 'r') as file:\n",
    "    carter_speech = json.load(file)\n",
    "\n",
    "with open('./speech_json/40_reagan_speech.json', 'r') as file:\n",
    "    reagan_speech = json.load(file)\n",
    "\n",
    "with open('./speech_json/41_herbertbush_speech.json', 'r') as file:\n",
    "    herbertbush_speech = json.load(file)\n",
    "\n",
    "with open('./speech_json/42_clinton_speech.json', 'r') as file:\n",
    "    clinton_speech = json.load(file)\n",
    "\n",
    "with open('./speech_json/43_walkerbush_speech.json', 'r') as file:\n",
    "    walkerbush_speech = json.load(file)\n",
    "\n",
    "with open('./speech_json/44_obama_speech.json', 'r') as file:\n",
    "    obama_speech = json.load(file)\n",
    "\n",
    "with open('./speech_json/45_trump_speech.json', 'r') as file:\n",
    "    trump_speech = json.load(file)\n",
    "\n",
    "with open('./speech_json/46_biden_speech.json', 'r') as file:\n",
    "    biden_speech = json.load(file)\n",
    "\n",
    "with open('./speech_json/47_vicepresident_biden_speech.json', 'r') as file:\n",
    "    vicepresident_biden_speech = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(speech_file, output_file):\n",
    "    results = []\n",
    "\n",
    "    for entry in speech_file:\n",
    "        sentence = entry['sentence']\n",
    "        date = entry['date']\n",
    "        tokens = tokenizer(sentence, truncation=True, max_length=512, return_tensors=\"pt\", padding=True)\n",
    "        truncated_sentence = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
    "        result = classify(truncated_sentence)\n",
    "        sentiment_score = result[0][0]['label'].split(' ')[0]\n",
    "\n",
    "        # Collect sentences with their scores and dates\n",
    "        results.append({\n",
    "            \"date\": date,\n",
    "            \"sentiment_score\": sentiment_score,\n",
    "            \"sentence\": sentence\n",
    "        })\n",
    "\n",
    "    # Save results to a JSON file\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(results, outfile, indent=4)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nixon_results = calculate_scores(nixon_speech, 'nixon_sentiment_scores.json')\n",
    "# ford_results = calculate_scores(ford_speech, 'ford_sentiment_scores.json')\n",
    "# carter_results = calculate_scores(carter_speech, 'carter_sentiment_scores.json')\n",
    "# reagan_results = calculate_scores(reagan_speech, 'reagan_sentiment_scores.json')\n",
    "# herbertbush_results = calculate_scores(herbertbush_speech, 'herbertbush_sentiment_scores.json')\n",
    "# clinton_results = calculate_scores(clinton_speech, 'clinton_sentiment_scores.json')\n",
    "# walkerbush_results = calculate_scores(walkerbush_speech, 'walkerbush_sentiment_scores.json')\n",
    "# obama_results = calculate_scores(obama_speech, 'obama_sentiment_scores.json')\n",
    "# trump_results = calculate_scores(trump_speech, 'trump_sentiment_scores.json')\n",
    "# biden_results = calculate_scores(biden_speech, 'biden_sentiment_scores.json')\n",
    "# vicepresident_biden_results = calculate_scores(vicepresident_biden_speech, 'vicepresident_biden_sentiment_scores.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./sentiment_scores_json/nixon_sentiment_scores.json', 'r') as file:\n",
    "    nixon_sentiment_scores = json.load(file)\n",
    "\n",
    "with open('./sentiment_scores_json/ford_sentiment_scores.json', 'r') as file:\n",
    "    ford_sentiment_scores = json.load(file)\n",
    "\n",
    "with open('./sentiment_scores_json/carter_sentiment_scores.json', 'r') as file:\n",
    "    carter_sentiment_scores = json.load(file)\n",
    "\n",
    "with open('./sentiment_scores_json/reagan_sentiment_scores.json', 'r') as file:\n",
    "    reagan_sentiment_scores = json.load(file)\n",
    "\n",
    "with open('./sentiment_scores_json/herbertbush_sentiment_scores.json', 'r') as file:\n",
    "    herbertbush_sentiment_scores = json.load(file)\n",
    "\n",
    "with open('./sentiment_scores_json/clinton_sentiment_scores.json', 'r') as file:\n",
    "    clinton_sentiment_scores = json.load(file)\n",
    "\n",
    "with open('./sentiment_scores_json/walkerbush_sentiment_scores.json', 'r') as file:\n",
    "    walkerbush_sentiment_scores = json.load(file)\n",
    "\n",
    "with open('./sentiment_scores_json/obama_sentiment_scores.json', 'r') as file:\n",
    "    obama_sentiment_scores = json.load(file)\n",
    "\n",
    "with open('./sentiment_scores_json/trump_sentiment_scores.json', 'r') as file:\n",
    "    trump_sentiment_scores = json.load(file)\n",
    "\n",
    "with open('./sentiment_scores_json/biden_sentiment_scores.json', 'r') as file:\n",
    "    biden_sentiment_scores = json.load(file)\n",
    "\n",
    "with open('./sentiment_scores_json/vicepresident_biden_sentiment_scores.json', 'r') as file:\n",
    "    vicepresident_biden_sentiment_scores = json.load(file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
