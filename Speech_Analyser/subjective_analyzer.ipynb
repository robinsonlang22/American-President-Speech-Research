{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a045b05c",
   "metadata": {},
   "source": [
    "### subjective/neutral of sentences analyzer\n",
    "1. import the json file consists of speechs include \"China\"\n",
    "2. use model to calculate the probability of subjective/neutral assign to sentences\n",
    "3. restore the result to the independent json files\n",
    "4. calculate the average, median and standard deviation of scores by president and year\n",
    "5. visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f57354eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers\n",
    "#pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a646b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5da4af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cffl/bert-base-styleclassification-subjective-neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48b97076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(json_file_path, max_length=512):\n",
    "    processed_sentences = []\n",
    "    \n",
    "    # read json file\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        sentences_data = json.load(file)\n",
    "    \n",
    "    # iterate each sentence\n",
    "    for entry in sentences_data:\n",
    "        sentence = entry['sentence']\n",
    "        \n",
    "        # tokenize and truncate\n",
    "        tokens = tokenizer(sentence, truncation=True, max_length=512, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # convert to text\n",
    "        truncated_sentence = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
    "        \n",
    "        # save processed sentence\n",
    "        processed_sentences.append(truncated_sentence)\n",
    "    \n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c081ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=\"cffl/bert-base-styleclassification-subjective-neutral\",\n",
    "    return_all_scores=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "242290fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./speech_json/37_nixon_speech.json', 'r') as file:\n",
    "#     nixon_speech = json.load(file)\n",
    "\n",
    "# with open('./speech_json/38_ford_speech.json', 'r') as file:\n",
    "#     ford_speech = json.load(file)\n",
    "\n",
    "# with open('./speech_json/39_carter_speech.json', 'r') as file:\n",
    "#     carter_speech = json.load(file)\n",
    "\n",
    "# with open('./speech_json/40_reagan_speech.json', 'r') as file:\n",
    "#     reagan_speech = json.load(file)\n",
    "\n",
    "# with open('./speech_json/41_herbertbush_speech.json', 'r') as file:\n",
    "#     herbertbush_speech = json.load(file)\n",
    "\n",
    "# with open('./speech_json/42_clinton_speech.json', 'r') as file:\n",
    "#     clinton_speech = json.load(file)\n",
    "\n",
    "# with open('./speech_json/43_walkerbush_speech.json', 'r') as file:\n",
    "#     walkerbush_speech = json.load(file)\n",
    "\n",
    "# with open('./speech_json/44_obama_speech.json', 'r') as file:\n",
    "#     obama_speech = json.load(file)\n",
    "\n",
    "# with open('./speech_json/45_trump_speech.json', 'r') as file:\n",
    "#     trump_speech = json.load(file)\n",
    "\n",
    "# with open('./speech_json/46_biden_speech.json', 'r') as file:\n",
    "#     biden_speech = json.load(file)\n",
    "\n",
    "# with open('./speech_json/47_vicepresident_biden_speech.json', 'r') as file:\n",
    "#     vicepresident_biden_speech = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9acfcaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(speech_file, output_file):\n",
    "    results = []\n",
    "\n",
    "    for entry in speech_file:\n",
    "        sentence = entry['sentence']\n",
    "        date = entry['date']\n",
    "        tokens = tokenizer(sentence, truncation=True, max_length=512, return_tensors=\"pt\", padding=True)\n",
    "        truncated_sentence = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
    "        result = classify(truncated_sentence)\n",
    "        subjective_score, neutral_score = result[0][0]['score'], result[0][1]['score']\n",
    "\n",
    "        # Collect sentences with their scores and dates\n",
    "        results.append({\n",
    "            \"date\": date,\n",
    "            \"subjective_score\": subjective_score,\n",
    "            \"neutral_score\": neutral_score,\n",
    "            \"sentence\": sentence\n",
    "        })\n",
    "\n",
    "    # Save results to a JSON file\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(results, outfile, indent=4)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14938cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nixon_results = calculate_scores(nixon_speech, 'nixon_scores.json')\n",
    "# ford_results = calculate_scores(ford_speech, 'ford_scores.json')\n",
    "# carter_results = calculate_scores(carter_speech, 'carter_scores.json')\n",
    "# reagan_results = calculate_scores(reagan_speech, 'reagan_scores.json')\n",
    "# herbertbush_results = calculate_scores(herbertbush_speech, 'herbertbush_scores.json')\n",
    "# clinton_results = calculate_scores(clinton_speech, 'clinton_scores.json')\n",
    "# walkerbush_results = calculate_scores(walkerbush_speech, 'walkerbush_scores.json')\n",
    "# obama_results = calculate_scores(obama_speech, 'obama_scores.json')\n",
    "# trump_results = calculate_scores(trump_speech, 'trump_scores.json')\n",
    "# biden_results = calculate_scores(biden_speech, 'biden_scores.json')\n",
    "# vicepresident_biden_results = calculate_scores(vicepresident_biden_speech, 'vicepresident_biden_scores.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f0924d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./subjective_scores_json/nixon_scores.json', 'r') as file:\n",
    "#     nixon_scores = json.load(file)\n",
    "\n",
    "# with open('./subjective_scores_json/ford_scores.json', 'r') as file:\n",
    "#     ford_scores = json.load(file)\n",
    "\n",
    "# with open('./subjective_scores_json/carter_scores.json', 'r') as file:\n",
    "#     carter_scores = json.load(file)\n",
    "\n",
    "# with open('./subjective_scores_json/reagan_scores.json', 'r') as file:\n",
    "#     reagan_scores = json.load(file)\n",
    "\n",
    "# with open('./subjective_scores_json/herbertbush_scores.json', 'r') as file:\n",
    "#     herbertbush_scores = json.load(file)\n",
    "\n",
    "# with open('./subjective_scores_json/clinton_scores.json', 'r') as file:\n",
    "#     clinton_scores = json.load(file)\n",
    "\n",
    "# with open('./subjective_scores_json/walkerbush_scores.json', 'r') as file:\n",
    "#     walkerbush_scores = json.load(file)\n",
    "\n",
    "# with open('./subjective_scores_json/obama_scores.json', 'r') as file:\n",
    "#     obama_scores = json.load(file)\n",
    "\n",
    "# with open('./subjective_scores_json/trump_scores.json', 'r') as file:\n",
    "#     trump_scores = json.load(file)\n",
    "\n",
    "# with open('./subjective_scores_json/biden_scores.json', 'r') as file:\n",
    "#     biden_scores = json.load(file)\n",
    "\n",
    "# with open('./subjective_scores_json/vicepresident_biden_scores.json', 'r') as file:\n",
    "#     vicepresident_biden_scores = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26347e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_calculator(scores, presidentname):\n",
    "\n",
    "    subjective_scores = [score['subjective_score'] for score in scores]\n",
    "    neutral_scores = [score['neutral_score'] for score in scores if score['neutral_score'] < 0.9]\n",
    "    \n",
    "    if not subjective_scores:\n",
    "        return None, None, None\n",
    "    \n",
    "    average_subjective_score = round(np.mean(subjective_scores), 3)\n",
    "    median_subjective_score = round(np.median(subjective_scores), 3)\n",
    "    std_dev_subjective_score = round(np.std(subjective_scores), 3)\n",
    "    average_neutral_score = round(np.mean(neutral_scores), 3)\n",
    "    median_neutral_score = round(np.median(neutral_scores), 3)\n",
    "    std_dev_neutral_score = round(np.std(neutral_scores), 3) \n",
    "\n",
    "    print(f\"president_name: {presidentname}\")\n",
    "    print(f\"average_subjective_score: {average_subjective_score:.3f}\")\n",
    "    print(f\"median_subjective_score: {median_subjective_score:.3f}\")\n",
    "    print(f\"std_dev_subjective_score: {std_dev_subjective_score:.3f}\")\n",
    "    print(f\"average_neutral_score: {average_neutral_score:.3f}\")\n",
    "    print(f\"median_neutral_score: {median_neutral_score:.3f}\")\n",
    "    print(f\"std_dev_neutral_score: {std_dev_neutral_score:.3f}\")\n",
    "\n",
    "    results = {\n",
    "        \"president_name\": presidentname,\n",
    "        \"average_subjective_score\": average_subjective_score,\n",
    "        \"median_subjective_score\": median_subjective_score,\n",
    "        \"std_dev_subjective_score\": std_dev_subjective_score,\n",
    "        \"average_neutral_score\": average_neutral_score,\n",
    "        \"median_neutral_score\": median_neutral_score,\n",
    "        \"std_dev_neutral_score\": std_dev_neutral_score\n",
    "    }\n",
    "\n",
    "    # Save results to a JSON file\n",
    "    with open('scores_statistics.json', 'a') as outfile:\n",
    "        json.dump(results, outfile, indent=4)\n",
    "    \n",
    "    return average_subjective_score, median_subjective_score, std_dev_subjective_score, average_neutral_score, median_neutral_score, std_dev_neutral_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7076ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nixon_statistics = statistics_calculator(nixon_scores, \"nixon\")\n",
    "# ford_statistics = statistics_calculator(ford_scores, \"ford\")\n",
    "# carter_statistics = statistics_calculator(carter_scores, \"carter\")\n",
    "# reagan_statistics = statistics_calculator(reagan_scores, \"reagan\")\n",
    "# herbertbush_statistics = statistics_calculator(herbertbush_scores, \"herbertbush\")\n",
    "# clinton_statistics = statistics_calculator(clinton_scores, \"clinton\")\n",
    "# walkerbush_statistics = statistics_calculator(walkerbush_scores, \"walkerbush\")\n",
    "# obama_statistics = statistics_calculator(obama_scores, \"obama\")\n",
    "# trump_statistics = statistics_calculator(trump_scores, \"trump\")\n",
    "# biden_statistics = statistics_calculator(biden_scores, \"biden\")\n",
    "# vicepresident_biden_statistics = statistics_calculator(vicepresident_biden_scores, \"vicepresident_biden\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc3998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "with open('./scores_statistics.json', 'r') as file:\n",
    "    scores_statistics = json.load(file)\n",
    "\n",
    "# Extract data from scores_statistics   \n",
    "presidents = [entry['president_name'] for entry in scores_statistics]\n",
    "average_subjective_scores = [entry['average_subjective_score'] for entry in scores_statistics]\n",
    "median_subjective_scores = [entry['median_subjective_score'] for entry in scores_statistics]\n",
    "average_neutral_scores = [entry['average_neutral_score'] for entry in scores_statistics]\n",
    "median_neutral_scores = [entry['median_neutral_score'] for entry in scores_statistics]\n",
    "\n",
    "# Create subplots (2 columns: 1 for Subjective Scores, 1 for Neutral Scores)\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Subjective Scores\", \"Neutral Scores\"))\n",
    "\n",
    "# Plot average and median subjective scores in the first subplot\n",
    "fig.add_trace(go.Scatter(x=presidents, y=average_subjective_scores, \n",
    "                         mode='lines+markers', name='Average Subjective Score',\n",
    "                         marker=dict(symbol='circle', color='blue')), \n",
    "              row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=presidents, y=median_subjective_scores, \n",
    "                         mode='lines+markers', name='Median Subjective Score',\n",
    "                         marker=dict(symbol='square', color='red')), \n",
    "              row=1, col=1)\n",
    "\n",
    "# Plot average and median neutral scores in the second subplot\n",
    "fig.add_trace(go.Scatter(x=presidents, y=average_neutral_scores, \n",
    "                         mode='lines+markers', name='Average Neutral Score',\n",
    "                         marker=dict(symbol='circle', color='green')), \n",
    "              row=1, col=2)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=presidents, y=median_neutral_scores, \n",
    "                         mode='lines+markers', name='Median Neutral Score',\n",
    "                         marker=dict(symbol='square', color='orange')), \n",
    "              row=1, col=2)\n",
    "\n",
    "# Update layout (rotate x-axis labels, add titles, etc.)\n",
    "fig.update_layout(\n",
    "    height=600, \n",
    "    width=1100, \n",
    "    title_text=\"Subjective and Neutral Sentiment Scores by President\",\n",
    "    title_font_size=24,\n",
    "    showlegend=True,\n",
    "    hovermode='closest',  # Ensures closest point to cursor triggers hover\n",
    "    xaxis=dict(tickangle=-45),\n",
    "    yaxis=dict(showline=True),\n",
    "    xaxis2=dict(tickangle=-45),\n",
    "    yaxis2=dict(showline=True),\n",
    ")\n",
    "\n",
    "fig.update_traces(hoverinfo='text', hovertemplate='%{x}<br>Score: %{y}', \n",
    "                  hoverlabel=dict(namelength=-1), \n",
    "                  marker=dict(symbol='circle', size=10),\n",
    "                  line=dict(width=2), \n",
    "                  mode='lines+markers')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "fig.update_xaxes(tickangle=-45)\n",
    "\n",
    "# Display the figure\n",
    "fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f794b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top10_sentences(scores):\n",
    "    subjective_sentences = []\n",
    "    neutral_sentences = []\n",
    "\n",
    "    for entry in scores:\n",
    "        sentence = entry['sentence']\n",
    "        subjective_score = entry['subjective_score']\n",
    "        neutral_score = entry['neutral_score']\n",
    "        if neutral_score < 0.9:\n",
    "            subjective_sentences.append((sentence, subjective_score))\n",
    "            neutral_sentences.append((sentence, neutral_score))\n",
    "\n",
    "    # Sort and print top 10 subjective and neutral sentences\n",
    "    subjective_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "    neutral_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\\nTop 10 Subjective Sentences:\")\n",
    "    for sentence, score in subjective_sentences[:10]:\n",
    "        print(f\"{score:.4f}: {sentence}\")\n",
    "\n",
    "    print(\"\\nTop 10 Neutral Sentences:\")\n",
    "    for sentence, score in neutral_sentences[:10]:\n",
    "        print(f\"{score:.4f}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d7105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top10_sentences(nixon_scores)\n",
    "# top10_sentences(ford_scores)\n",
    "# top10_sentences(carter_scores)\n",
    "# top10_sentences(reagan_scores)\n",
    "# top10_sentences(herbertbush_scores)\n",
    "# top10_sentences(clinton_scores)\n",
    "# top10_sentences(walkerbush_scores)\n",
    "# top10_sentences(obama_scores)\n",
    "# top10_sentences(trump_scores)\n",
    "# top10_sentences(biden_scores)\n",
    "# top10_sentences(vicepresident_biden_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
